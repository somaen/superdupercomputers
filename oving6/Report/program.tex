\section{The program}
\subsection{Components of a solution}
Our program consists of three components that come together to produce a
results to the problem at hand, a matrix that holds the actual data, and some
necessary metadata, like row/column-count, a transpose-function for this
matrix, that utilizes MPI for communication, and finally the provided (inverse)
fast sine transform-functions.

\subsubsection{The matrix}
We can set the matrix as the minimum memory we need as we do not attempt to
compress the information. The matrix is square and this results in it using
\emph{n} times the space a variable occupies. After finalising we saw that we
should have linked our struct directly to the MPI session as it never is used
without it and that would make the code somewhat more neat. 

\subsubsection{MPI and transposition}
MPI is only used to infer the local dimensions of the matrix and more directly
to do the communication during the transpose operation. We use what we consider
the minimum practical space required for this, auxillary memory being the same
size as the area where the matrix holds the data. To minimise the connections
needed we serialise the data before sending so that all data going from process
A and process B is in one sequence. After the data have been received in a
process it will do a transpose locally, since it must be done at one point and
after the transport step is the most convenient point to do it. 

Technically we could have managed with less auxiliary memory, but we believe
this would have resulted in longer running time, because we would need to do
parts of the transposition before we can continue the communication and thus
add much more complexity. 

\subsubsection{fast sine transform and inverse }
As the fortran file was not quite readable for us, we neatly wrapped up the
\texttt{fst\_()} and \texttt{fstinv\_()} functions in functions which allocates the
auxillary memory needed and dispatches the function which does the math. These
wrappers are trivially parallelisable using \emph{openMP}, the one thing to
watch out for is the part where each paralell need its own auxillary space.
Paralellising uses \emph{n} times the size of a double for each parallel run at
the same time, a low price to pay for the speed increase. 

\subsection{Putting it all together} 
By looking at the original program and figuring out what each part did we
mapped that to what we had implemented and put it together the same way, just
using our parallelised versions. The result looks more or less like this list,
except for timers that we included. 

\begin{itemize}
\item Initialise the matrix
\item Initialise the eigenvalue vector to our operator
\item apply the fast sine transform on the matrix
\item transpose the matrix
\item apply the fast inverse sine transform
\item divide the matrix elements by the eigenvalues of the operator 
\item sine transform
\item transpose the matrix
\item inverse sine transform to get the answer
\end{itemize}
